{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fc3de-2cde-44e5-918e-c1f2dc2038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import argparse\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)\n",
    "\n",
    "def parse_args(config):\n",
    "    print(\"Running with the following config\")\n",
    "    parser = argparse.ArgumentParser(description='Run training baseline')\n",
    "    for k,v in config.__dict__.items():\n",
    "        parser.add_argument('--'+k, type=type(v) if type(v) is not bool else str2bool, \n",
    "                            default=v, \n",
    "                            help=f\"Default: {v}\")\n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    # update config with parsed args\n",
    "    for k, v in args.items():\n",
    "        try:\n",
    "            # attempt to eval it it (e.g. if bool, number, or etc)\n",
    "            attempt = literal_eval(v)\n",
    "        except (SyntaxError, ValueError):\n",
    "            # if that goes wrong, just use the string\n",
    "            attempt = v\n",
    "        setattr(config, k, attempt)\n",
    "        print(f\"--{k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34d6d7-e0a3-4dbf-a664-c0116e3336a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "WANDB_PROJECT = \"alpaca-ft\"\n",
    "WANDB_ENTITY = \"kevinv3796\"\n",
    "INPUT_DATASET_TABLE_AT = dict(\n",
    "    table_at = 'kevinv3796/alpaca-ft/run-8paqy77d-eval_predictions:latest', \n",
    "    table_name = \"eval_predictions\"\n",
    ")\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    max_tokens=256,\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    temperature=1,\n",
    "    prompt_col=\"prompt\",\n",
    "    input_dataset_table_at=INPUT_DATASET_TABLE_AT,\n",
    "    output_table=\"gpt35_results\",\n",
    "    num_samples=-1,  # for debug purposes\n",
    ")\n",
    "\n",
    "def generate_35(prompt, config):\n",
    "    completion = completion_with_backoff(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": config.system_prompt},\n",
    "                  {\"role\": \"user\",\"content\": prompt}\n",
    "                 ],\n",
    "        max_tokens=config.max_tokens,\n",
    "        temperature=config.temperature,\n",
    "    \n",
    "    )\n",
    "    output = completion.choices[0].message.content\n",
    "    # print(f\"##PROMPT: {prompt}\\n##COMPLETION: {output}\")\n",
    "    return output\n",
    "\n",
    "def download_table(table_at, table_name):\n",
    "    artifact = wandb.use_artifact(table_at, type='run_table')\n",
    "    artifact_dir = artifact.download()\n",
    "    table = artifact.get(table_name)\n",
    "    df = pd.DataFrame(data=table.data, columns=table.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "parse_args(config)\n",
    "# create a run to have lineage\n",
    "wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, job_type=\"eval\", tags=[\"gpt-3.5\"], config=config)\n",
    "    \n",
    "input_df = download_table(**config.input_dataset_table_at)\n",
    "results = []\n",
    "for i, prompt in enumerate(tqdm(input_df[config.prompt_col].to_list()[:config.num_samples])):\n",
    "    results.append((i, generate_35(prompt, config)))\n",
    "gpt35_df = pd.DataFrame(results, columns=[\"index\", \"generation\"])\n",
    "\n",
    "# format to save\n",
    "gpt35_df = gpt35_df.assign(prompt=input_df[config.prompt_col])\n",
    "gpt35_df = gpt35_df[[\"index\", \"prompt\", \"generation\"]]\n",
    "print(gpt35_df.head())\n",
    "    \n",
    "gpt35_table = wandb.Table(dataframe=gpt35_df)\n",
    "    \n",
    "# save our work's output!\n",
    "wandb.log({config.output_table:gpt35_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366d77f-a340-477f-b03e-f070da2d63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from llm_recipes.utils import parse_args\n",
    "from llm_recipes.openai import completion_with_backoff\n",
    "\n",
    "WANDB_PROJECT = \"alpaca-ft\"\n",
    "WANDB_ENTITY = \"kevinv3796\"\n",
    "\n",
    "FT_MODEL_PREDS_AT = dict(\n",
    "    table_at='capecape/alpaca_ft/run-oxql1s2a-eval_predictions:v0',\n",
    "    table_name=\"eval_predictions\",\n",
    ")\n",
    "GPT35_MODEL_PREDS_AT = dict(\n",
    "    table_at='capecape/alpaca_ft/run-vkr1q6bw-gpt35_results:v0',\n",
    "    table_name='gpt35_results',\n",
    ")\n",
    "\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    openai_model = \"gpt-4\",\n",
    "    temperature=1,\n",
    "    system_prompt = (\"You will be presented with a choice of two possible responses for an instruction\"\n",
    "                     \"You have to pick the best one and give a reason why.\\n\"\n",
    "                     \"The reponse should follow the instructions and use the provided context if there is some\\n\"\n",
    "                     \"If both answers are equivalent, pick the value 0\"),\n",
    "    model_names=[\"ft_model\", \"gpt35\"],\n",
    "    num_samples=-1,\n",
    "    out_dir=\"./output\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def download_table(table_at, table_name):\n",
    "    artifact = wandb.use_artifact(table_at, type='run_table')\n",
    "    artifact_dir = artifact.download()\n",
    "    table = artifact.get(table_name)\n",
    "    df = pd.DataFrame(data=table.data, columns=table.columns)\n",
    "    return df\n",
    "\n",
    "def gpt4_judge(instruction, gen1, gen2, model=config.openai_model, system_prompt=config.system_prompt):\n",
    "    message = \"{instruction}\\n Answer 1: \\n{gen1}\\n Answer 2:\\n{gen2}\".format(instruction=instruction, gen1=gen1, gen2=gen2)\n",
    "    completion = completion_with_backoff(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\",\n",
    "                   \"content\": system_prompt,\n",
    "                  },\n",
    "                  {\"role\": \"user\",\n",
    "                   \"content\": message,\n",
    "                  },],\n",
    "        function_call = {\"name\": \"make_choice\"},\n",
    "        functions = [\n",
    "            {\n",
    "                \"name\": \"make_choice\",\n",
    "                \"description\": \"Select the best generation and argument why\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"choice\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"the choosen alternative, zero if equivalent\",\n",
    "                        },\n",
    "                        \"reason\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Reason why the choice was made\",\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                    \"required\": [\"choice\", \"reason\"],\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def judge_row(row, model1_name, model2_name):\n",
    "    \"Judge with inversion of prompt order (2x more expensive)\"\n",
    "    prompt = row.prompt\n",
    "    gen1 = row[model1_name]\n",
    "    gen2 = row[model2_name]\n",
    "    res = gpt4_judge(prompt, gen1, gen2)\n",
    "    res_inverted = gpt4_judge(prompt, gen2, gen1)\n",
    "    return res, res_inverted\n",
    "\n",
    "def extract_function_call(res):\n",
    "    \"Extract the function call arguments\"\n",
    "    try:\n",
    "        response = json.loads(res.choices[0].message.function_call.arguments)\n",
    "        choice = response[\"choice\"]\n",
    "        reason = response[\"reason\"]\n",
    "    except:\n",
    "        choice = -1\n",
    "        reason = \"gpt4 fail\"\n",
    "    return choice, reason\n",
    "\n",
    "def judge(merged_df, model1_name, model2_name):\n",
    "    \"Apply the judge stuff!\"\n",
    "    gpt4_results = []\n",
    "    for i in tqdm(range(len(merged_df))):\n",
    "        row = merged_df.iloc[i]\n",
    "        res, res_inverted = judge_row(row, model1_name, model2_name)\n",
    "        choice, reason = extract_function_call(res)\n",
    "        print(choice, reason)\n",
    "        choice_inverted, reason_inverted = extract_function_call(res_inverted)\n",
    "        print(choice_inverted, reason_inverted)\n",
    "        agree = (choice - choice_inverted) % 2 == 1\n",
    "        # print(f\"GPT4 prefers: {choice} and {choice_inverted}: Agree={agree}\")\n",
    "        gpt4_results.append((i, row.prompt, row[model1_name], row[model2_name], choice, choice_inverted, reason, reason_inverted))\n",
    "    return gpt4_results\n",
    "\n",
    "def agree_check(row):\n",
    "    \"Check where the GPT4 agrees with himself\"\n",
    "    if row.choice == 0 and row.choice_inverted==0:\n",
    "        return True\n",
    "    elif ((row.choice - row.choice_inverted) % 2 == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parse_args(config)\n",
    "    out_dir = Path(config.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # create a run to have lineage\n",
    "    wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, job_type=\"eval\", tags=[\"gpt-4\"], config=config)\n",
    "    \n",
    "    gpt35_df = download_table(**GPT35_MODEL_PREDS_AT)\n",
    "    ft_results_df = download_table(**FT_MODEL_PREDS_AT)\n",
    "\n",
    "    # merge both\n",
    "    merged_df = pd.merge(\n",
    "        ft_results_df[[\"prompt\", \"generation\"]], \n",
    "        gpt35_df[[\"prompt\", \"generation\"]], on=\"prompt\", suffixes=config.model_names,\n",
    "    )\n",
    "    model1_name, model2_name = (f\"generation{s}\" for s in config.model_names)\n",
    "    gpt_results = judge(merged_df.iloc[:config.num_samples], model1_name, model2_name)\n",
    "    results_df = pd.DataFrame(\n",
    "        gpt_results,\n",
    "        columns=[\"index\", \"prompt\", config.model_names[0], config.model_names[1], \n",
    "                 \"choice\", \"choice_inverted\", \"reason\", \"reason_inverted\"]).set_index(\"index\")\n",
    "    results_df[\"agree\"] = results_df.apply(agree_check, axis=1)\n",
    "\n",
    "    final_results = results_df[results_df.agree]\n",
    "    print(f\"The judge agrees on: {len(final_results)} / {len(ft_results_df)}\")\n",
    "    \n",
    "    # Let's log those also:\n",
    "    disagree_df = results_df[~results_df.agree]\n",
    "\n",
    "    # Lets use a better naming than 0,1,2\n",
    "    choices = [\"both\",] + config.model_names\n",
    "    final_results[\"choice_name\"] = [choices[c] for c in final_results[\"choice\"]]\n",
    "    print(\"\\n### GPT JUDGE RESULTS ###\")\n",
    "    print(\"###########################\")\n",
    "    print(final_results[\"choice_name\"].value_counts())\n",
    "    print(\"###########################\")\n",
    "\n",
    "    final_results.to_csv(out_dir/\"gpt4_eval.csv\")\n",
    "    gpt4_table = wandb.Table(dataframe=final_results)\n",
    "    wandb.log({\"gpt4_eval\":gpt4_table})   \n",
    "\n",
    "    disagree_df[\"choice\"] = [choices[c] for c in disagree_df[\"choice\"]]\n",
    "    choices_inverted = [\"both\",] + config.model_names[::-1]\n",
    "    disagree_df[\"choice_inverted\"] = [choices_inverted[c] for c in disagree_df[\"choice_inverted\"]]\n",
    "\n",
    "    disagree_df.to_csv(out_dir/\"gpt4_eval_disagree.csv\")\n",
    "    gpt4_eval_disagree_table  = wandb.Table(dataframe=disagree_df)\n",
    "    wandb.log({\"gpt4_eval_disagree\":gpt4_eval_disagree_table})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d4282-e24a-4e8f-99b6-802e55e27fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4_judge(instruction, gen1, gen2, model=\"gpt-4\"):\n",
    "    system_prompt = (\"You will be presented with a choice of two possible responses for an instruction\"\n",
    "                     \"You have to pick the best one and give a reason why.\\n\"\n",
    "                     \"The reponse should follow the instructions and use the provided context if there is some\\n\"\n",
    "                    \"If both answers are equivalent, pick the value 0\")\n",
    "    message = \"{instruction}\\n Answer 1: \\n{gen1}\\n Answer 2:\\n{gen2}\".format(instruction=instruction, gen1=gen1, gen2=gen2)\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\",\n",
    "                   \"content\": system_prompt,\n",
    "                  },\n",
    "                  {\"role\": \"user\",\n",
    "                   \"content\": message,\n",
    "                  },],\n",
    "        function_call = {\"name\": \"make_choice\"},\n",
    "        functions = [{\n",
    "                \"name\": \"make_choice\",\n",
    "                \"description\": \"Select the best generation and explain why\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"choice\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"the choosen alternative, zero if equivalent\",\n",
    "                        },\n",
    "                        \"argument\":{\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Reason why the choice was made\",},},},\n",
    "                    \"required\": [\"choice\", \"argument\"],},\n",
    "        ],)\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9586a9-ac59-42c0-825b-99a58e6fb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear each environment variable\n",
    "for var in variables_to_clear:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n",
    "# Verify that the environment variables are cleared\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))  # Should print None if cleared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
